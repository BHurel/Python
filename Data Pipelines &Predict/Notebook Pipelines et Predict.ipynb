{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Notebook Data Pipelines et Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 classes:**\n",
    "- Data Handler\n",
    "- Feature Recipe\n",
    "- Future Extactor\n",
    "- Model Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:\n",
    "- https://storage.googleapis.com/h3-data/listings_final.csv\n",
    "- https://storage.googleapis.com/h3-data/price_availability.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe DataHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 attributs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer vos librairies  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-399a45084aa8>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-399a45084aa8>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    self.df_price_availability pd.read_csv(\"Data/price_availabilty.csv\", sep=\";\")\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self):\n",
    "        self.df_listing_final = None\n",
    "        self.df_price_availability = None\n",
    "        self.df_merge = None\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.df_listings_final = pd.read_csv(\"Data/listings_final.csv\", sep=\";\", index_col=0)\n",
    "        self.df_price_availability pd.read_csv(\"Data/price_availabilty.csv\", sep=\";\")\n",
    "        \n",
    "    def group_data(self):\n",
    "        #merge \n",
    "        self.df_merge = pd.merge(self.df_listings)\n",
    "    def get_process_data(self):\n",
    "        self.get_data()\n",
    "        self.group_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    \"\"\"\n",
    "        Get data from GSC Bucket \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.csvfile1 = None\n",
    "        self.csvfile2 = None\n",
    "        self.gouped_data = None\n",
    "    def get_data(self):\n",
    "        print(\" - - - fetch data from gcs bucket : - - - \")\n",
    "        listing = pd.read_csv('https://storage.googleapis.com/h3-data/listings_final.csv', sep=';')\n",
    "        prices = pd.read_csv('https://storage.googleapis.com/h3-data/price_availability.csv', sep=';')\n",
    "        self.csvfile1, self.csvfile2 = listing, prices  \n",
    "        return \" - - - data loaded - - - \\nFiles : \\n  - listing {} \\n  - prices {}\".format(listing.shape,prices.shape)\n",
    "    def group_data(self):\n",
    "        data = self.csvfile2.groupby('listing_id')['local_price'].mean()\n",
    "        self.gouped_data = pd.merge(data, self.csvfile1, on='listing_id')\n",
    "        print(\" - - - data merged - - - \")\n",
    "    def get_process_data(self):\n",
    "        self.get_data()\n",
    "        self.group_data()\n",
    "        print(\" - - - data processed - - - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0e566b70f572>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-807b84d32f02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_process_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "d.get_process_data()\n",
    "%time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe FutureRecipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-d11532fe3d55>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-d11532fe3d55>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    \"\"\" TODO : Diviser les types de variables dans un tableau \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class FeatureRecipe:\n",
    "    \"\"\"\n",
    "    Feature processing class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "        self.continuous = None\n",
    "        self.categorical = None\n",
    "        self.discrete = None\n",
    "        self.datetime = None\n",
    "        \n",
    "    #Separate features for feature engineering\n",
    "    def separate_vraible_types(self) -> None:\n",
    "    \"\"\" TODO : Diviser les types de variables dans un tableau \"\"\"\n",
    "        pass   \n",
    "    \n",
    "    #Useless feature\n",
    "    def drop_uselessf(self):\n",
    "    \"\"\" TODO : Supprimer les colonnes inutiles du dataset \"\"\"\n",
    "    #Soit celles en double soit celles avec Na de 100%\n",
    "        pass\n",
    "    \n",
    "    def deal_duplicate(self):\n",
    "    \"\"\" TODO : Supprimer les lignes dupliquÃ©es du dataset \"\"\"\n",
    "        pass\n",
    "    \n",
    "    #NaN\n",
    "    def drop_nanp(self, thresold: float):\n",
    "    \"\"\" TODO : Supprimer un pourcentage de NA du dataset \"\"\"\n",
    "        pass\n",
    "    \n",
    "    #Datetime\n",
    "    def deal_dtime(self):\n",
    "    \"\"\" TODO : Traiter les DateTime \"\"\"\n",
    "        pass    \n",
    "    #\n",
    "    # extract data (optionnal)\n",
    "    #\n",
    "    \n",
    "    def prepare_data(self, threshold: float):\n",
    "        self.separate_variable_types()\n",
    "        self.drop_uselessf()\n",
    "        self.deal_duplicate()\n",
    "        self.drop_nanp(threshold)\n",
    "        self.deal_dtime()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct Drop NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nanp(self, thresold: float):\n",
    "        \"\"\"\n",
    "        Drop NaN columns according to a thresold\n",
    "        \"\"\"\n",
    "        def deal_nanp(df:pd.DataFrame, thresold: float):\n",
    "            bf=[]\n",
    "            for c in self.data.columns.to_list():\n",
    "                if self.data[c].isna().sum()/self.data.shape[0] > thresold:\n",
    "                    bf.append(c)\n",
    "            logging.debug(\"{} feature have more than {} NaN \".format(len(bf), thresold))\n",
    "            logging.debug('\\n\\n - - - features - - -  \\n {}'.format(bf))\n",
    "            return bf \n",
    "        self.data = self.data.drop(deal_nanp(self.data, thresold), axis=1)\n",
    "        logging.debug('Some NaN features droped according to {} thresold'.format(thresold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe FeatureExtractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Feature Extractor class\n",
    "    \"\"\"    \n",
    "    def __init__(self, data: pd.DataFrame, flist: list):\n",
    "        \"\"\"\n",
    "            Input : pandas.DataFrame, feature list to drop\n",
    "            Output : X_train, X_test, y_train, y_test according to sklearn.model_selection.train_test_split\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
